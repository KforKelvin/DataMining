{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.1: Find log-likelihood\n",
    "import numpy as np\n",
    "\n",
    "D = np.array([1.0, 1.3, 2.2, 2.6, 2.8, 5.0, 7.3, 7.4, 7.5, 7.7])\n",
    "\n",
    "mu_1, sigma_1, P_C1 = 6.63, 1, 0.5\n",
    "mu_2, sigma_2, P_C2 = 7.57, 1, 0.5\n",
    "\n",
    "# Guassian PDF (Lecture 08 page 11)\n",
    "def Guasian_PDF(mu,sigma,x):\n",
    "    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2) \n",
    "\n",
    "# log likelihood (L8 Page 21)\n",
    "log_likelihood = 0\n",
    "\n",
    "for x in D:\n",
    "    P1 = P_C1 * Guasian_PDF(mu_1, sigma_1,x)\n",
    "    P2 = P_C2 * Guasian_PDF(mu_2, sigma_2,x)\n",
    "\n",
    "    log_likelihood += np.log(P1 + P2)\n",
    "\n",
    "print(log_likelihood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2 (Lecture 8 Page 25)\n",
    "D = np.array([1.0, 1.3, 2.2, 2.6, 2.8, 5.0, 7.3, 7.4, 7.5, 7.7])\n",
    "\n",
    "mu_1, sigma_1, P_C1 = 6.63, 1, 0.5\n",
    "mu_2, sigma_2, P_C2 = 7.57, 1, 0.5\n",
    "\n",
    "# Guassian PDF (Lecture 08 page 11)\n",
    "def Guasian_PDF(mu,sigma,x):\n",
    "    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2) \n",
    "\n",
    "# log likelihood (L8 Page 21)\n",
    "result = []\n",
    "\n",
    "for x in D:\n",
    "    P1 = P_C1 * Guasian_PDF(mu_1, sigma_1,x)\n",
    "    P2 = P_C2 * Guasian_PDF(mu_2, sigma_2,x)\n",
    "\n",
    "    w_i1 = P1 / (P1 + P2)  # posterior probabilities for cluster 1\n",
    "    w_i2 = P2 / (P1 + P2)  # posterior probabilities for cluster 2\n",
    "\n",
    "    result.append((w_i1,w_i2))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.3\n",
    "import numpy as np\n",
    "\n",
    "D = np.array([1.0, 1.3, 2.2, 2.6, 2.8, 5.0, 7.3, 7.4, 7.5, 7.7])\n",
    "\n",
    "# posterior probabilities\n",
    "result = np.array([\n",
    "    [0.9968\t, 0.0032],\n",
    "    [0.9957\t, 0.0043],\n",
    "    [0.9901\t, 0.0099],\n",
    "    [0.9857\t, 0.0143],\n",
    "    [0.9827\t, 0.0173],\n",
    "    [0.878\t, 0.122],\n",
    "    [0.4531\t, 0.5469],\n",
    "    [0.43\t, 0.57],\n",
    "    [0.4071\t, 0.5929],\n",
    "    [0.3626\t, 0.6374]\n",
    "])\n",
    "\n",
    "#  Means\n",
    "def update_means(D, result):\n",
    "\n",
    "    resp_sum_1 = np.sum(result[:, 0])\n",
    "    resp_sum_2 = np.sum(result[:, 1])\n",
    "    \n",
    "    mu_1 = np.sum(result[:, 0] * D) / resp_sum_1\n",
    "    mu_2 = np.sum(result[:, 1] * D) / resp_sum_2\n",
    "    \n",
    "    return mu_1, mu_2\n",
    "\n",
    "#  Variances\n",
    "def update_variances(D, result, mu_1, mu_2):\n",
    "    resp_sum_1 = np.sum(result[:, 0])\n",
    "    resp_sum_2 = np.sum(result[:, 1])\n",
    "    \n",
    "    sigma_1_sq = np.sum(result[:, 0] * (D - mu_1)**2) / resp_sum_1\n",
    "    sigma_2_sq = np.sum(result[:, 1] * (D - mu_2)**2) / resp_sum_2\n",
    "    \n",
    "    return sigma_1_sq, sigma_2_sq\n",
    "\n",
    "# Mixture Weights\n",
    "def update_mixture_weights(result):\n",
    "    N = result.shape[0]\n",
    "    P_C1 = np.sum(result[:, 0]) / N\n",
    "    P_C2 = np.sum(result[:, 1]) / N\n",
    "    \n",
    "    return P_C1, P_C2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2] \n",
    "y = iris.target      \n",
    "\n",
    "# Q4.1\n",
    "# plot\n",
    "for target, color, label in zip(np.unique(y), ['r', 'g', 'b'], iris.target_names):\n",
    "    plt.scatter(X[y == target, 1], X[y == target, 0], c=color, label=label)\n",
    "plt.xlabel('Sepal Width (cm)')\n",
    "plt.ylabel('Sepal Length (cm)')\n",
    "plt.title('Sepal Width vs. Sepal Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4.2a\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 3 clusters, and random_state  = 0\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 1], X[:, 0], c=kmeans_labels)\n",
    "plt.xlabel('Sepal Width (cm)')\n",
    "plt.ylabel('Sepal Length (cm)')\n",
    "plt.title('K-Means Clustering - Sepal Measurements')\n",
    "plt.show()\n",
    "\n",
    "#Q4.2b\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_index = silhouette_score(X, kmeans_labels)\n",
    "print(\"Silhouette Index: \",silhouette_index)\n",
    "\n",
    "\n",
    "# Q4.2c\n",
    "'''\n",
    "According to the class label plot (P1), we can see some green dots and blue dots \n",
    "are mixed together, or overlapping. After we do cluster assignment (P2), we kinda\n",
    "of split the overlapping situation better. The samples are now clearly separated \n",
    "into 3 different sectors. \n",
    "\n",
    "Silhouette index is a value between -1 and 1. Negative values generally indicate that a \n",
    "sample has been assigned to the wrong cluster. Values near 0 indicate overlapping \n",
    "clusters. 0.445 means moderate clustering structure. So yes, as we can see from P1, we \n",
    "can tell there are some mixture/overlapping in vertisicolor and virginica.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.3\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2] \n",
    "y = iris.target \n",
    "\n",
    "silhouette_score_set = []\n",
    "\n",
    "for k in range(2,51):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "\n",
    "    silhouette_index = silhouette_score(X, kmeans_labels)\n",
    "    silhouette_score_set.append((k,float(silhouette_index)))\n",
    "\n",
    "# print(silhouette_score_set)\n",
    "\n",
    "k_values = [item[0] for item in silhouette_score_set]\n",
    "silhouette_scores = [item[1] for item in silhouette_score_set]\n",
    "\n",
    "\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs. Number of Clusters')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Yes, K = 2 is better than K = 3. Because from the P1, based sepal measurements alone,\n",
    "the data naturally forms two clusters, which is why we saw some overlapping in vertisicolor and virginica\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.4\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# Apply K-Medoids clustering\n",
    "kmedoids = KMedoids(n_clusters=3, random_state=0)\n",
    "kmedoids_labels = kmedoids.fit_predict(X)\n",
    "\n",
    "# Plot the clustering results\n",
    "# plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 1], X[:, 0], c=kmedoids_labels)\n",
    "plt.ylabel('Sepal Length (cm)')\n",
    "plt.xlabel('Sepal Width (cm)')\n",
    "plt.title('K-Medoids Clustering (k=3) - Sepal Measurements')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "K-mediod looks almost the same as k-means.\n",
    "KMedoids is related to the KMeans algorithm. While KMeans \n",
    "tries to minimize the within cluster sum-of-squares, KMedoids\n",
    "tries to minimize the sum of distances between each point and the medoid of its cluster. \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
